{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64ad9fb",
   "metadata": {},
   "source": [
    "# Aircraft Modification Data Preprocessing\n",
    "\n",
    "This notebook demonstrates comprehensive data preprocessing for aircraft modification descriptions, including:\n",
    "- Environment setup and data loading\n",
    "- Text cleaning and normalization\n",
    "- Feature extraction using TF-IDF and SBERT\n",
    "- Aviation-specific pattern recognition\n",
    "- Data visualization and analysis\n",
    "\n",
    "**Goal**: Prepare aircraft modification data for machine learning models that will classify modifications, map regulations, and predict certification requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bea1f5",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports\n",
    "\n",
    "First, let's install and import all required libraries for text processing, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ef88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running for the first time)\n",
    "# !pip install pandas numpy scikit-learn nltk spacy transformers sentence-transformers\n",
    "# !pip install plotly seaborn matplotlib streamlit faiss-cpu\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Deep learning and transformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Transformers not available. Some features will be limited.\")\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append('../utils')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"ü§ñ Transformers available: {TRANSFORMERS_AVAILABLE}\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845196d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk_downloads = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words']\n",
    "\n",
    "for dataset in nltk_downloads:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{dataset}')\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.data.find(f'corpora/{dataset}')\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.data.find(f'taggers/{dataset}')\n",
    "            except LookupError:\n",
    "                try:\n",
    "                    nltk.data.find(f'chunkers/{dataset}')\n",
    "                except LookupError:\n",
    "                    nltk.download(dataset, quiet=True)\n",
    "\n",
    "# Set up matplotlib and seaborn\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úÖ NLTK data downloaded and configurations set!\")\n",
    "print(\"üìà Visualization libraries configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9118aa",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "Load the aircraft modification dataset and perform initial exploration to understand the data structure, quality, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f952d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aircraft modification dataset\n",
    "data_path = '../data/mods_dataset.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found. Let's create sample data...\")\n",
    "    # Generate sample data if the file doesn't exist\n",
    "    from generate_sample_data import SampleDataGenerator\n",
    "    \n",
    "    generator = SampleDataGenerator()\n",
    "    df = generator.generate_dataset(100)\n",
    "    df.to_csv(data_path, index=False)\n",
    "    print(\"‚úÖ Sample dataset created and saved!\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Total modifications: {len(df)}\")\n",
    "print(f\"Date range: {df['approval_date'].min()} to {df['approval_date'].max()}\")\n",
    "print(f\"Unique modification types: {df['mod_type'].nunique()}\")\n",
    "print(f\"Unique aircraft types: {df['aircraft_type'].nunique()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Data types and missing values\n",
    "print(\"\\nüìä Data Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical overview\n",
    "print(\"üìà STATISTICAL OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        print(f\"  {col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  {col}: ‚úÖ No missing values\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Categorical Variables:\")\n",
    "categorical_cols = ['mod_type', 'loi', 'aircraft_type']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        value_counts = df[col].value_counts()\n",
    "        for value, count in value_counts.head(10).items():\n",
    "            print(f\"  {value}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Text length analysis\n",
    "print(\"\\nüìù Text Analysis:\")\n",
    "df['description_length'] = df['mod_description'].str.len()\n",
    "df['word_count'] = df['mod_description'].str.split().str.len()\n",
    "df['regulation_count'] = df['regulations'].str.split(',').str.len()\n",
    "\n",
    "text_stats = {\n",
    "    'Description Length (chars)': df['description_length'].describe(),\n",
    "    'Word Count': df['word_count'].describe(),\n",
    "    'Regulation Count': df['regulation_count'].describe()\n",
    "}\n",
    "\n",
    "for stat_name, stats in text_stats.items():\n",
    "    print(f\"\\n{stat_name}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.1f}\")\n",
    "    print(f\"  Median: {stats['50%']:.1f}\")\n",
    "    print(f\"  Min: {stats['min']:.0f}, Max: {stats['max']:.0f}\")\n",
    "    print(f\"  Std: {stats['std']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194beb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for data exploration\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Modification Types', 'Level of Involvement', \n",
    "                   'Text Length Distribution', 'Regulations per Modification'),\n",
    "    specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "           [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    ")\n",
    "\n",
    "# Modification types\n",
    "mod_counts = df['mod_type'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=mod_counts.values, y=mod_counts.index, orientation='h', \n",
    "           name='Mod Types', showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Level of Involvement\n",
    "loi_counts = df['loi'].value_counts()\n",
    "colors = ['green', 'orange', 'red']\n",
    "fig.add_trace(\n",
    "    go.Bar(x=loi_counts.index, y=loi_counts.values,\n",
    "           marker=dict(color=colors[:len(loi_counts)]),\n",
    "           name='LOI', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Text length distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=df['description_length'], nbinsx=20,\n",
    "                name='Text Length', showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Regulations per modification\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=df['regulation_count'], nbinsx=10,\n",
    "                name='Regulation Count', showlegend=False),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Aircraft Modification Dataset Overview\")\n",
    "fig.show()\n",
    "\n",
    "# Word cloud of modification descriptions (if wordcloud available)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Combine all descriptions\n",
    "    all_text = ' '.join(df['mod_description'])\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=100).generate(all_text)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Most Common Words in Modification Descriptions', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"WordCloud not available. Install with: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000fdf2",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing and Cleaning\n",
    "\n",
    "Implement comprehensive text preprocessing specifically designed for aircraft modification descriptions, including:\n",
    "- Text cleaning and normalization\n",
    "- Aviation-specific pattern recognition\n",
    "- Tokenization and lemmatization\n",
    "- Stop word removal with domain-specific additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AviationTextPreprocessor:\n",
    "    \"\"\"Advanced text preprocessor for aviation domain\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Standard stop words + aviation-specific terms\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        aviation_stopwords = {\n",
    "            'aircraft', 'airplane', 'flight', 'aviation', 'system', 'systems',\n",
    "            'installation', 'installed', 'modify', 'modification', 'mod',\n",
    "            'equipment', 'component', 'device', 'unit', 'assembly', 'new',\n",
    "            'improved', 'enhanced', 'advanced', 'latest', 'current'\n",
    "        }\n",
    "        self.stop_words.update(aviation_stopwords)\n",
    "        \n",
    "        # Aviation-specific regex patterns\n",
    "        self.patterns = {\n",
    "            'regulations': r'\\b(CS|AMC)\\s*[\\d\\-\\.]+\\b',\n",
    "            'part_numbers': r'\\b[A-Z]{2,4}[\\d\\-]{3,10}\\b',\n",
    "            'aircraft_models': r'\\b(A\\d{3}|B\\d{3}|ATR|CRJ|ERJ)\\w*\\b',\n",
    "            'measurements': r'\\d+\\s*(mm|cm|m|ft|in|kg|lb|psi|bar|kts|mach)\\b',\n",
    "            'frequencies': r'\\d+\\s*(hz|khz|mhz|ghz)\\b',\n",
    "            'voltages': r'\\d+\\s*(v|volt|volts|vdc|vac)\\b'\n",
    "        }\n",
    "        \n",
    "        # Technical term mapping\n",
    "        self.tech_terms = {\n",
    "            'vhf': 'very_high_frequency',\n",
    "            'uhf': 'ultra_high_frequency',\n",
    "            'gps': 'global_positioning_system',\n",
    "            'ils': 'instrument_landing_system',\n",
    "            'tcas': 'traffic_collision_avoidance_system',\n",
    "            'egpws': 'enhanced_ground_proximity_warning_system'\n",
    "        }\n",
    "    \n",
    "    def extract_aviation_entities(self, text):\n",
    "        \"\"\"Extract aviation-specific entities from text\"\"\"\n",
    "        entities = {}\n",
    "        \n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            entities[entity_type] = list(set(matches))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Comprehensive text cleaning for aviation domain\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Store original for entity extraction\n",
    "        original_text = text\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace technical abbreviations with full terms\n",
    "        for abbrev, full_term in self.tech_terms.items():\n",
    "            text = re.sub(rf'\\b{abbrev}\\b', full_term, text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove URLs and email addresses\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Keep only letters, numbers, and important punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\-\\.\\(\\)/]', ' ', text)\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_and_process(self, text, use_lemmatization=True):\n",
    "        \"\"\"Tokenize and process text with domain-specific handling\"\"\"\n",
    "        # Clean text first\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        \n",
    "        # Filter tokens\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # Skip if too short or is punctuation\n",
    "            if len(token) < 2 or token in string.punctuation:\n",
    "                continue\n",
    "            \n",
    "            # Skip stop words\n",
    "            if token.lower() in self.stop_words:\n",
    "                continue\n",
    "            \n",
    "            # Apply lemmatization or stemming\n",
    "            if use_lemmatization:\n",
    "                token = self.lemmatizer.lemmatize(token)\n",
    "            else:\n",
    "                token = self.stemmer.stem(token)\n",
    "            \n",
    "            processed_tokens.append(token)\n",
    "        \n",
    "        return processed_tokens\n",
    "    \n",
    "    def analyze_text_complexity(self, text):\n",
    "        \"\"\"Analyze text complexity and characteristics\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        analysis = {\n",
    "            'sentence_count': len(sentences),\n",
    "            'word_count': len(words),\n",
    "            'avg_sentence_length': len(words) / len(sentences) if sentences else 0,\n",
    "            'unique_words': len(set(word.lower() for word in words)),\n",
    "            'lexical_diversity': len(set(word.lower() for word in words)) / len(words) if words else 0,\n",
    "            'avg_word_length': np.mean([len(word) for word in words]) if words else 0\n",
    "        }\n",
    "        \n",
    "        # Extract aviation entities\n",
    "        entities = self.extract_aviation_entities(text)\n",
    "        analysis['aviation_entities'] = entities\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = AviationTextPreprocessor()\n",
    "\n",
    "print(\"‚úÖ Aviation Text Preprocessor initialized!\")\n",
    "print(\"üîß Features available:\")\n",
    "print(\"  - Text cleaning and normalization\")\n",
    "print(\"  - Aviation-specific entity extraction\")\n",
    "print(\"  - Domain-aware tokenization\")\n",
    "print(\"  - Text complexity analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee232df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing on sample texts\n",
    "sample_texts = [\n",
    "    \"Installation of a new VHF antenna on the dorsal fuselage affecting structural and avionics systems according to CS 25.1309.\",\n",
    "    \"Retrofit of LED cabin lighting system replacing existing fluorescent lights with 28VDC power supply.\",\n",
    "    \"Modification of TCAS II system for enhanced collision avoidance using AMC 20-151 guidelines.\",\n",
    "    \"Integration of GPS/WAAS navigation system in A320 aircraft for RNAV approaches per AMC 20-115.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ PREPROCESSING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"\\nüìù Sample {i}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned = preprocessor.clean_text(text)\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = preprocessor.tokenize_and_process(text)\n",
    "    print(f\"Tokens:   {tokens[:10]}...\")  # Show first 10 tokens\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = preprocessor.extract_aviation_entities(text)\n",
    "    print(f\"Entities: {entities}\")\n",
    "    \n",
    "    # Analyze complexity\n",
    "    analysis = preprocessor.analyze_text_complexity(text)\n",
    "    print(f\"Analysis: Words={analysis['word_count']}, \"\n",
    "          f\"Unique={analysis['unique_words']}, \"\n",
    "          f\"Diversity={analysis['lexical_diversity']:.2f}\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c68aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the entire dataset\n",
    "print(\"üîÑ Processing entire dataset...\")\n",
    "\n",
    "# Create processed versions of the text\n",
    "df['description_cleaned'] = df['mod_description'].apply(preprocessor.clean_text)\n",
    "df['description_tokens'] = df['mod_description'].apply(preprocessor.tokenize_and_process)\n",
    "df['token_count'] = df['description_tokens'].apply(len)\n",
    "\n",
    "# Extract aviation entities for each modification\n",
    "print(\"üîç Extracting aviation entities...\")\n",
    "aviation_entities = []\n",
    "for desc in df['mod_description']:\n",
    "    entities = preprocessor.extract_aviation_entities(desc)\n",
    "    aviation_entities.append(entities)\n",
    "\n",
    "df['aviation_entities'] = aviation_entities\n",
    "\n",
    "# Analyze text complexity\n",
    "print(\"üìä Analyzing text complexity...\")\n",
    "complexity_data = []\n",
    "for desc in df['mod_description']:\n",
    "    analysis = preprocessor.analyze_text_complexity(desc)\n",
    "    complexity_data.append(analysis)\n",
    "\n",
    "# Convert to separate columns\n",
    "complexity_df = pd.DataFrame(complexity_data)\n",
    "df = pd.concat([df, complexity_df], axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìà PREPROCESSING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Processed {len(df)} modifications\")\n",
    "print(f\"üìù Average tokens per description: {df['token_count'].mean():.1f}\")\n",
    "print(f\"üî§ Average word length: {df['avg_word_length'].mean():.1f}\")\n",
    "print(f\"üìö Lexical diversity range: {df['lexical_diversity'].min():.2f} - {df['lexical_diversity'].max():.2f}\")\n",
    "\n",
    "# Entity extraction summary\n",
    "entity_types = ['regulations', 'part_numbers', 'aircraft_models', 'measurements']\n",
    "print(f\"\\nüè∑Ô∏è ENTITY EXTRACTION SUMMARY\")\n",
    "for entity_type in entity_types:\n",
    "    count = sum(1 for entities in aviation_entities if entities.get(entity_type))\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  {entity_type}: {count} modifications ({percentage:.1f}%)\")\n",
    "\n",
    "# Save processed data\n",
    "processed_path = '../data/mods_processed.csv'\n",
    "df.to_csv(processed_path, index=False)\n",
    "print(f\"\\nüíæ Processed data saved to: {processed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53364156",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction and Vectorization\n",
    "\n",
    "Create numerical features from the processed text using multiple approaches:\n",
    "- TF-IDF vectorization for traditional ML models\n",
    "- Sentence embeddings using SBERT for semantic similarity\n",
    "- Custom feature engineering for aviation domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "print(\"üî¢ TF-IDF VECTORIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare text data for TF-IDF\n",
    "texts_for_tfidf = df['description_cleaned'].tolist()\n",
    "\n",
    "# Create TF-IDF vectorizer with optimized parameters for aviation domain\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,          # Limit vocabulary size\n",
    "    ngram_range=(1, 2),         # Use unigrams and bigrams\n",
    "    min_df=2,                   # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.8,                 # Ignore terms that appear in more than 80% of documents\n",
    "    stop_words='english',       # Use English stop words\n",
    "    lowercase=True,\n",
    "    strip_accents='ascii',\n",
    "    token_pattern=r'\\b[a-zA-Z][a-zA-Z0-9_]{2,}\\b'  # Custom token pattern\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "print(\"üîÑ Fitting TF-IDF vectorizer...\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_for_tfidf)\n",
    "\n",
    "print(f\"‚úÖ TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"üìä Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"üíæ Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.1f}%\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Find most important features\n",
    "feature_importance = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "top_features_idx = np.argsort(feature_importance)[::-1][:20]\n",
    "\n",
    "print(f\"\\nüîù Top 20 TF-IDF features:\")\n",
    "for i, idx in enumerate(top_features_idx, 1):\n",
    "    print(f\"  {i:2d}. {feature_names[idx]} (score: {feature_importance[idx]:.2f})\")\n",
    "\n",
    "# Save TF-IDF components\n",
    "import pickle\n",
    "tfidf_path = '../models/tfidf_vectorizer.pkl'\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "with open(tfidf_path, 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print(f\"\\nüíæ TF-IDF vectorizer saved to: {tfidf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe62bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Embeddings using SBERT\n",
    "print(\"\\nü§ñ SENTENCE EMBEDDINGS (SBERT)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    try:\n",
    "        # Load sentence transformer model\n",
    "        print(\"üîÑ Loading sentence transformer model...\")\n",
    "        model_name = 'all-MiniLM-L6-v2'  # Lightweight but effective model\n",
    "        sentence_model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Generate embeddings for all descriptions\n",
    "        print(\"üîÑ Generating sentence embeddings...\")\n",
    "        sentence_embeddings = sentence_model.encode(\n",
    "            df['description_cleaned'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Sentence embeddings shape: {sentence_embeddings.shape}\")\n",
    "        print(f\"üìè Embedding dimension: {sentence_embeddings.shape[1]}\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        embeddings_path = '../models/sentence_embeddings.npy'\n",
    "        np.save(embeddings_path, sentence_embeddings)\n",
    "        \n",
    "        print(f\"üíæ Sentence embeddings saved to: {embeddings_path}\")\n",
    "        \n",
    "        # Calculate embedding statistics\n",
    "        embedding_stats = {\n",
    "            'mean': np.mean(sentence_embeddings),\n",
    "            'std': np.std(sentence_embeddings),\n",
    "            'min': np.min(sentence_embeddings),\n",
    "            'max': np.max(sentence_embeddings)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä Embedding statistics:\")\n",
    "        for stat, value in embedding_stats.items():\n",
    "            print(f\"  {stat}: {value:.4f}\")\n",
    "        \n",
    "        # Visualize embeddings using t-SNE (sample for performance)\n",
    "        if len(sentence_embeddings) > 100:\n",
    "            sample_idx = np.random.choice(len(sentence_embeddings), 100, replace=False)\n",
    "            sample_embeddings = sentence_embeddings[sample_idx]\n",
    "            sample_labels = df.iloc[sample_idx]['mod_type'].values\n",
    "        else:\n",
    "            sample_embeddings = sentence_embeddings\n",
    "            sample_labels = df['mod_type'].values\n",
    "        \n",
    "        print(f\"\\nüé® Creating t-SNE visualization with {len(sample_embeddings)} samples...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(sample_embeddings)-1))\n",
    "        embeddings_2d = tsne.fit_transform(sample_embeddings)\n",
    "        \n",
    "        # Create interactive plot\n",
    "        fig = px.scatter(\n",
    "            x=embeddings_2d[:, 0], \n",
    "            y=embeddings_2d[:, 1],\n",
    "            color=sample_labels,\n",
    "            title=\"t-SNE Visualization of Sentence Embeddings\",\n",
    "            labels={'x': 't-SNE 1', 'y': 't-SNE 2', 'color': 'Modification Type'}\n",
    "        )\n",
    "        fig.update_traces(marker_size=8)\n",
    "        fig.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating sentence embeddings: {e}\")\n",
    "        sentence_embeddings = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Sentence transformers not available. Skipping sentence embeddings.\")\n",
    "    sentence_embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Feature Engineering for Aviation Domain\n",
    "print(\"\\nüõ†Ô∏è CUSTOM FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def create_aviation_features(df):\n",
    "    \"\"\"Create domain-specific features for aircraft modifications\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Text-based features\n",
    "    features['text_length'] = df['mod_description'].str.len()\n",
    "    features['word_count'] = df['word_count']\n",
    "    features['sentence_count'] = df['sentence_count']\n",
    "    features['avg_word_length'] = df['avg_word_length']\n",
    "    features['lexical_diversity'] = df['lexical_diversity']\n",
    "    features['token_count'] = df['token_count']\n",
    "    \n",
    "    # Aviation entity features\n",
    "    features['has_regulations'] = df['aviation_entities'].apply(\n",
    "        lambda x: len(x.get('regulations', [])) > 0\n",
    "    )\n",
    "    features['regulation_count'] = df['aviation_entities'].apply(\n",
    "        lambda x: len(x.get('regulations', []))\n",
    "    )\n",
    "    features['has_part_numbers'] = df['aviation_entities'].apply(\n",
    "        lambda x: len(x.get('part_numbers', [])) > 0\n",
    "    )\n",
    "    features['has_aircraft_models'] = df['aviation_entities'].apply(\n",
    "        lambda x: len(x.get('aircraft_models', [])) > 0\n",
    "    )\n",
    "    features['has_measurements'] = df['aviation_entities'].apply(\n",
    "        lambda x: len(x.get('measurements', [])) > 0\n",
    "    )\n",
    "    \n",
    "    # Keyword-based features (binary indicators)\n",
    "    aviation_keywords = {\n",
    "        'safety_related': ['emergency', 'safety', 'evacuation', 'fire', 'oxygen', 'escape'],\n",
    "        'avionics_related': ['radio', 'radar', 'navigation', 'gps', 'antenna', 'communication'],\n",
    "        'structural_related': ['wing', 'fuselage', 'door', 'frame', 'structural', 'reinforcement'],\n",
    "        'system_related': ['hydraulic', 'fuel', 'air', 'conditioning', 'pump', 'valve'],\n",
    "        'cabin_related': ['passenger', 'seat', 'galley', 'lavatory', 'lighting', 'entertainment'],\n",
    "        'propulsion_related': ['engine', 'thrust', 'propulsion', 'turbine', 'combustor']\n",
    "    }\n",
    "    \n",
    "    for category, keywords in aviation_keywords.items():\n",
    "        features[f'has_{category}'] = df['description_cleaned'].apply(\n",
    "            lambda text: any(keyword in text.lower() for keyword in keywords)\n",
    "        )\n",
    "        features[f'count_{category}'] = df['description_cleaned'].apply(\n",
    "            lambda text: sum(text.lower().count(keyword) for keyword in keywords)\n",
    "        )\n",
    "    \n",
    "    # Complexity features\n",
    "    features['technical_density'] = (\n",
    "        features['regulation_count'] + \n",
    "        features['count_avionics_related'] + \n",
    "        features['count_system_related']\n",
    "    ) / features['word_count']\n",
    "    \n",
    "    # Modification urgency indicators\n",
    "    urgency_keywords = ['critical', 'urgent', 'immediate', 'mandatory', 'required']\n",
    "    features['urgency_score'] = df['description_cleaned'].apply(\n",
    "        lambda text: sum(text.lower().count(keyword) for keyword in urgency_keywords)\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Generate custom features\n",
    "print(\"üîÑ Generating aviation-specific features...\")\n",
    "aviation_features = create_aviation_features(df)\n",
    "\n",
    "print(f\"‚úÖ Created {aviation_features.shape[1]} custom features\")\n",
    "print(f\"üìä Feature summary:\")\n",
    "\n",
    "# Display feature statistics\n",
    "feature_stats = aviation_features.describe()\n",
    "print(feature_stats.round(2))\n",
    "\n",
    "# Correlation analysis\n",
    "print(f\"\\nüîó Feature correlations with modification type:\")\n",
    "# Encode mod_type for correlation\n",
    "le = LabelEncoder()\n",
    "mod_type_encoded = le.fit_transform(df['mod_type'])\n",
    "\n",
    "correlations = []\n",
    "for col in aviation_features.select_dtypes(include=[np.number]).columns:\n",
    "    corr = np.corrcoef(aviation_features[col], mod_type_encoded)[0, 1]\n",
    "    if not np.isnan(corr):\n",
    "        correlations.append((col, abs(corr)))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 features correlated with modification type:\")\n",
    "for i, (feature, corr) in enumerate(correlations[:10], 1):\n",
    "    print(f\"  {i:2d}. {feature}: {corr:.3f}\")\n",
    "\n",
    "# Save custom features\n",
    "features_path = '../models/aviation_features.csv'\n",
    "aviation_features.to_csv(features_path, index=False)\n",
    "print(f\"\\nüíæ Custom features saved to: {features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803d052",
   "metadata": {},
   "source": [
    "## 5. Multi-label Encoding for Regulations\n",
    "\n",
    "Prepare regulation data for multi-label classification, where each modification can be associated with multiple regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b728a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label Encoding for Regulations\n",
    "print(\"üè∑Ô∏è MULTI-LABEL REGULATION ENCODING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Parse regulations from comma-separated strings\n",
    "def parse_regulations(reg_string):\n",
    "    \"\"\"Parse comma-separated regulation string into list\"\"\"\n",
    "    if pd.isna(reg_string) or reg_string == '':\n",
    "        return []\n",
    "    \n",
    "    regulations = [reg.strip() for reg in reg_string.split(',')]\n",
    "    # Clean up regulations (remove empty strings)\n",
    "    regulations = [reg for reg in regulations if reg]\n",
    "    return regulations\n",
    "\n",
    "# Apply parsing to all regulations\n",
    "df['regulation_list'] = df['regulations'].apply(parse_regulations)\n",
    "\n",
    "# Analyze regulation distribution\n",
    "all_regulations = []\n",
    "for reg_list in df['regulation_list']:\n",
    "    all_regulations.extend(reg_list)\n",
    "\n",
    "regulation_counts = Counter(all_regulations)\n",
    "print(f\"üìä Total unique regulations: {len(regulation_counts)}\")\n",
    "print(f\"üìà Total regulation instances: {len(all_regulations)}\")\n",
    "print(f\"üî¢ Average regulations per modification: {len(all_regulations) / len(df):.1f}\")\n",
    "\n",
    "# Display most common regulations\n",
    "print(f\"\\nüîù Top 15 most common regulations:\")\n",
    "for i, (regulation, count) in enumerate(regulation_counts.most_common(15), 1):\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  {i:2d}. {regulation}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Create multi-label binary encoding\n",
    "print(f\"\\nüîÑ Creating multi-label binary encoding...\")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "regulation_binary = mlb.fit_transform(df['regulation_list'])\n",
    "\n",
    "print(f\"‚úÖ Binary encoding shape: {regulation_binary.shape}\")\n",
    "print(f\"üìã Encoded regulations: {len(mlb.classes_)}\")\n",
    "\n",
    "# Create DataFrame with regulation columns\n",
    "regulation_df = pd.DataFrame(\n",
    "    regulation_binary, \n",
    "    columns=[f\"reg_{reg.replace(' ', '_').replace('.', '_')}\" for reg in mlb.classes_],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "print(f\"üìä Regulation matrix sparsity: {(1 - regulation_binary.sum() / regulation_binary.size) * 100:.1f}%\")\n",
    "\n",
    "# Analyze regulation co-occurrence\n",
    "print(f\"\\nüîó Regulation co-occurrence analysis:\")\n",
    "\n",
    "# Calculate pairwise correlations for top regulations\n",
    "top_regs = regulation_counts.most_common(10)\n",
    "top_reg_names = [reg for reg, _ in top_regs]\n",
    "\n",
    "# Create correlation matrix for top regulations\n",
    "top_reg_cols = [f\"reg_{reg.replace(' ', '_').replace('.', '_')}\" for reg in top_reg_names]\n",
    "available_cols = [col for col in top_reg_cols if col in regulation_df.columns]\n",
    "\n",
    "if len(available_cols) > 1:\n",
    "    corr_matrix = regulation_df[available_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        center=0,\n",
    "        square=True,\n",
    "        fmt='.2f'\n",
    "    )\n",
    "    plt.title('Regulation Co-occurrence Correlation Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save multi-label encoding components\n",
    "mlb_path = '../models/regulation_mlb.pkl'\n",
    "with open(mlb_path, 'wb') as f:\n",
    "    pickle.dump(mlb, f)\n",
    "\n",
    "regulation_binary_path = '../models/regulation_binary.npy'\n",
    "np.save(regulation_binary_path, regulation_binary)\n",
    "\n",
    "print(f\"\\nüíæ Multi-label binarizer saved to: {mlb_path}\")\n",
    "print(f\"üíæ Binary regulation matrix saved to: {regulation_binary_path}\")\n",
    "\n",
    "# Add regulation features to main dataframe\n",
    "df['total_regulations'] = df['regulation_list'].apply(len)\n",
    "df['regulation_diversity'] = df['regulation_list'].apply(lambda x: len(set(x)))\n",
    "\n",
    "print(f\"\\nüìã Regulation statistics added to main DataFrame:\")\n",
    "print(f\"  Total regulations per mod: {df['total_regulations'].describe()}\")\n",
    "print(f\"  Regulation diversity per mod: {df['regulation_diversity'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b23a3",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "‚úÖ **Data Loading & Exploration**\n",
    "- Loaded and analyzed aircraft modification dataset\n",
    "- Explored data distribution and characteristics\n",
    "- Identified key patterns and statistics\n",
    "\n",
    "‚úÖ **Text Preprocessing**\n",
    "- Implemented aviation-specific text cleaning\n",
    "- Created domain-aware tokenization and lemmatization\n",
    "- Extracted aviation entities (regulations, part numbers, etc.)\n",
    "\n",
    "‚úÖ **Feature Engineering**\n",
    "- Generated TF-IDF vectors for traditional ML models\n",
    "- Created sentence embeddings using SBERT for semantic similarity\n",
    "- Built custom aviation-domain features\n",
    "- Implemented multi-label encoding for regulations\n",
    "\n",
    "‚úÖ **Data Preparation**\n",
    "- Processed text for machine learning models\n",
    "- Created multiple feature representations\n",
    "- Saved preprocessed data and model components\n",
    "\n",
    "### Generated Assets\n",
    "\n",
    "üìÅ **Saved Files:**\n",
    "- `../data/mods_processed.csv` - Preprocessed dataset\n",
    "- `../models/tfidf_vectorizer.pkl` - TF-IDF vectorizer\n",
    "- `../models/sentence_embeddings.npy` - SBERT embeddings\n",
    "- `../models/aviation_features.csv` - Custom features\n",
    "- `../models/regulation_mlb.pkl` - Multi-label binarizer\n",
    "- `../models/regulation_binary.npy` - Binary regulation matrix\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "üéØ **Ready for Model Development:**\n",
    "\n",
    "1. **Modification Classification** (`2_mod_classification.ipynb`)\n",
    "   - Use TF-IDF + custom features\n",
    "   - Train Random Forest and Logistic Regression models\n",
    "   - Evaluate classification performance\n",
    "\n",
    "2. **Regulation Mapping** (`3_regulation_mapping.ipynb`)\n",
    "   - Multi-label classification for regulation prediction\n",
    "   - Use binary relevance and multi-label KNN approaches\n",
    "   - Evaluate using Hamming loss and Precision@K\n",
    "\n",
    "3. **LOI Prediction** (`4_loi_prediction.ipynb`)\n",
    "   - Predict Level of Involvement using all features\n",
    "   - Try Decision Tree, Random Forest, and XGBoost\n",
    "   - Analyze feature importance\n",
    "\n",
    "4. **Similarity Search** (`5_similarity_search.ipynb`)\n",
    "   - Build FAISS index from sentence embeddings\n",
    "   - Implement cosine similarity search\n",
    "   - Create similarity ranking system\n",
    "\n",
    "The preprocessing pipeline is now complete and ready for model development! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
